{"content":"<p>本节主要就伪代码来演示pipline parallel如何实现, 主要参考是Megatron-LM中的\ndistribute pp部分</p>\n<p><a href=\"https://developer.aliyun.com/article/1644595\" target=\"_blank\" rel=\"noopener noreferrer\">一篇讲gpipe和pipedream的文章</a></p>\n<p>在Megatron-LM中，pipline parallel schedule的选择是通\n过<code v-pre>get_forward_backward_func()</code>来实现的，会根据当前的配置选择适合的调度策略，然\n后完整地进行一次batch的fwd和bwd，因为调度策略分为很多，例如1F1B-interleaved,\ndualpipev..., 所以这里可以学习dualpipe的方式，定义一个通用的Schedule基类, 此后的\n调度策略通过实现这个基类来进行</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">class</span><span style=\"--shiki-light:#2E8F82;--shiki-dark:#5DA994\"> BaseScheduleStrategy</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ABC</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">    Base class for all schedule strategies</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> __init__</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">classmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> initialize_pipline_distributed</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cls</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Initialize the pipline parallel distributed strategy hooks or utils</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">classmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> get_stage_layer_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cls</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> num_layers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> int</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> -></span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> List</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">List</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">int</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]]:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Get the layer ids for each stage</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> __call__</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Call the schedule strategy</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>在实际的pp中，需要考虑更多的细节，例如tensor的释放与回收，这里我们讨论train的\npp，因为对于inference来说，只需要执行fwd的过程，简单了很多。</p>\n<p>首先我们需要明确两个点：</p>\n<ol>\n<li>在fwd阶段，我们需要的是上一个阶段的<code v-pre>input_tensor</code>, 然后获\n得<code v-pre>output_tensor = fwd(input_tensor)</code>, <code v-pre>output_tensor</code>发送给下一个stage作\n为<code v-pre>input_tensor</code>, 这里<code v-pre>input_tensor</code>和<code v-pre>output_tensor</code>都应该压入栈中等待fwd，\n发送完之后，<code v-pre>output_tensor</code>的数据其实已经不需要了，因为由下一个stage\n的<code v-pre>input_tensor</code>来保存，但是我们仍然需要其<code v-pre>grad_fn</code>之类的元数据来执行bwd, 所\n以这里进行deallocate(伪释放)，不是del, 而是替换其数据字段为一个标量。</li>\n</ol>\n<p>总结：<code v-pre>input_tensor</code>: 完全保留，<code v-pre>output_tensor</code>: 保留grad_fn</p>\n<ol start=\"2\">\n<li>在bwd阶段，我们需要的是上一个阶段传来的<code v-pre>output_tensor_grad</code>, 然后根据当前阶段\n的<code v-pre>input_tensor</code>和<code v-pre>output_tensor</code>, 计算出<code v-pre>input_tensor_grad</code>，其\n中<code v-pre>output_tensor</code>已经经过伪释放，只保留了计算图信息，然后通过bwd计算出\n了<code v-pre>input_tensor_grad</code>, 存储在<code v-pre>input_tensor</code>中，我们需要将<code v-pre>input_tensor</code>中\n的<code v-pre>grad</code>取出来，send出去, 之后<code v-pre>input_tensor</code>和<code v-pre>output_tensor_grad</code>本身也就没\n用了，完全销毁掉。</li>\n</ol>\n<p>总结: <code v-pre>input_tensor</code>: 计算完bwd之后完全释放，<code v-pre>output_tensor</code>: 计算完梯度之后完全\n释放，<code v-pre>output_tensor_grad</code>: 计算完梯度之后完全释放，<code v-pre>input_tensor_grad</code>, send之\n后完全释放</p>\n","env":{"base":"/","filePath":"/Users/zhangfan/zfan2356/github/AI-Router/ai-router-v2/docs/notes/system/pre-train/pp_communication.md","filePathRelative":"notes/system/pre-train/pp_communication.md","frontmatter":{"title":"Pipline Communication","author":"zfan","createTime":"2025/04/11 22:07:39","permalink":"/system/pre-train/pp/","tags":["system","pre-train"]},"sfcBlocks":{"template":{"type":"template","content":"<template><p>本节主要就伪代码来演示pipline parallel如何实现, 主要参考是Megatron-LM中的\ndistribute pp部分</p>\n<p><a href=\"https://developer.aliyun.com/article/1644595\" target=\"_blank\" rel=\"noopener noreferrer\">一篇讲gpipe和pipedream的文章</a></p>\n<p>在Megatron-LM中，pipline parallel schedule的选择是通\n过<code v-pre>get_forward_backward_func()</code>来实现的，会根据当前的配置选择适合的调度策略，然\n后完整地进行一次batch的fwd和bwd，因为调度策略分为很多，例如1F1B-interleaved,\ndualpipev..., 所以这里可以学习dualpipe的方式，定义一个通用的Schedule基类, 此后的\n调度策略通过实现这个基类来进行</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">class</span><span style=\"--shiki-light:#2E8F82;--shiki-dark:#5DA994\"> BaseScheduleStrategy</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ABC</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">    Base class for all schedule strategies</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> __init__</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">classmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> initialize_pipline_distributed</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cls</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Initialize the pipline parallel distributed strategy hooks or utils</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">classmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> get_stage_layer_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cls</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> num_layers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> int</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> -></span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> List</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">List</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">int</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]]:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Get the layer ids for each stage</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> __call__</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Call the schedule strategy</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>在实际的pp中，需要考虑更多的细节，例如tensor的释放与回收，这里我们讨论train的\npp，因为对于inference来说，只需要执行fwd的过程，简单了很多。</p>\n<p>首先我们需要明确两个点：</p>\n<ol>\n<li>在fwd阶段，我们需要的是上一个阶段的<code v-pre>input_tensor</code>, 然后获\n得<code v-pre>output_tensor = fwd(input_tensor)</code>, <code v-pre>output_tensor</code>发送给下一个stage作\n为<code v-pre>input_tensor</code>, 这里<code v-pre>input_tensor</code>和<code v-pre>output_tensor</code>都应该压入栈中等待fwd，\n发送完之后，<code v-pre>output_tensor</code>的数据其实已经不需要了，因为由下一个stage\n的<code v-pre>input_tensor</code>来保存，但是我们仍然需要其<code v-pre>grad_fn</code>之类的元数据来执行bwd, 所\n以这里进行deallocate(伪释放)，不是del, 而是替换其数据字段为一个标量。</li>\n</ol>\n<p>总结：<code v-pre>input_tensor</code>: 完全保留，<code v-pre>output_tensor</code>: 保留grad_fn</p>\n<ol start=\"2\">\n<li>在bwd阶段，我们需要的是上一个阶段传来的<code v-pre>output_tensor_grad</code>, 然后根据当前阶段\n的<code v-pre>input_tensor</code>和<code v-pre>output_tensor</code>, 计算出<code v-pre>input_tensor_grad</code>，其\n中<code v-pre>output_tensor</code>已经经过伪释放，只保留了计算图信息，然后通过bwd计算出\n了<code v-pre>input_tensor_grad</code>, 存储在<code v-pre>input_tensor</code>中，我们需要将<code v-pre>input_tensor</code>中\n的<code v-pre>grad</code>取出来，send出去, 之后<code v-pre>input_tensor</code>和<code v-pre>output_tensor_grad</code>本身也就没\n用了，完全销毁掉。</li>\n</ol>\n<p>总结: <code v-pre>input_tensor</code>: 计算完bwd之后完全释放，<code v-pre>output_tensor</code>: 计算完梯度之后完全\n释放，<code v-pre>output_tensor_grad</code>: 计算完梯度之后完全释放，<code v-pre>input_tensor_grad</code>, send之\n后完全释放</p>\n</template>","contentStripped":"<p>本节主要就伪代码来演示pipline parallel如何实现, 主要参考是Megatron-LM中的\ndistribute pp部分</p>\n<p><a href=\"https://developer.aliyun.com/article/1644595\" target=\"_blank\" rel=\"noopener noreferrer\">一篇讲gpipe和pipedream的文章</a></p>\n<p>在Megatron-LM中，pipline parallel schedule的选择是通\n过<code v-pre>get_forward_backward_func()</code>来实现的，会根据当前的配置选择适合的调度策略，然\n后完整地进行一次batch的fwd和bwd，因为调度策略分为很多，例如1F1B-interleaved,\ndualpipev..., 所以这里可以学习dualpipe的方式，定义一个通用的Schedule基类, 此后的\n调度策略通过实现这个基类来进行</p>\n<div class=\"language-python line-numbers-mode\" data-highlighter=\"shiki\" data-ext=\"python\" style=\"--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212\"><pre class=\"shiki shiki-themes vitesse-light vitesse-dark vp-code\" v-pre=\"\"><code><span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">class</span><span style=\"--shiki-light:#2E8F82;--shiki-dark:#5DA994\"> BaseScheduleStrategy</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#A65E2B;--shiki-dark:#C99076\">ABC</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">    Base class for all schedule strategies</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">    \"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> __init__</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">classmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> initialize_pipline_distributed</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cls</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Initialize the pipline parallel distributed strategy hooks or utils</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">classmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\"> get_stage_layer_ids</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">cls</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> num_layers</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">:</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> int</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">)</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\"> -></span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\"> List</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">List</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">[</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\">int</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">]]:</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Get the layer ids for each stage</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">    @</span><span style=\"--shiki-light:#59873A;--shiki-dark:#80A665\">abstractmethod</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\">    def</span><span style=\"--shiki-light:#998418;--shiki-dark:#B8A965\"> __call__</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">(</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">self</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> *</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">args</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">,</span><span style=\"--shiki-light:#AB5959;--shiki-dark:#CB7676\"> **</span><span style=\"--shiki-light:#393A34;--shiki-dark:#DBD7CAEE\">kwargs</span><span style=\"--shiki-light:#999999;--shiki-dark:#666666\">):</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B56959;--shiki-dark:#C98A7D\">        Call the schedule strategy</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#B5695977;--shiki-dark:#C98A7D77\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"--shiki-light:#1E754F;--shiki-dark:#4D9375\">        pass</span></span></code></pre>\n<div class=\"line-numbers\" aria-hidden=\"true\" style=\"counter-reset:line-number 0\"><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div><div class=\"line-number\"></div></div></div><p>在实际的pp中，需要考虑更多的细节，例如tensor的释放与回收，这里我们讨论train的\npp，因为对于inference来说，只需要执行fwd的过程，简单了很多。</p>\n<p>首先我们需要明确两个点：</p>\n<ol>\n<li>在fwd阶段，我们需要的是上一个阶段的<code v-pre>input_tensor</code>, 然后获\n得<code v-pre>output_tensor = fwd(input_tensor)</code>, <code v-pre>output_tensor</code>发送给下一个stage作\n为<code v-pre>input_tensor</code>, 这里<code v-pre>input_tensor</code>和<code v-pre>output_tensor</code>都应该压入栈中等待fwd，\n发送完之后，<code v-pre>output_tensor</code>的数据其实已经不需要了，因为由下一个stage\n的<code v-pre>input_tensor</code>来保存，但是我们仍然需要其<code v-pre>grad_fn</code>之类的元数据来执行bwd, 所\n以这里进行deallocate(伪释放)，不是del, 而是替换其数据字段为一个标量。</li>\n</ol>\n<p>总结：<code v-pre>input_tensor</code>: 完全保留，<code v-pre>output_tensor</code>: 保留grad_fn</p>\n<ol start=\"2\">\n<li>在bwd阶段，我们需要的是上一个阶段传来的<code v-pre>output_tensor_grad</code>, 然后根据当前阶段\n的<code v-pre>input_tensor</code>和<code v-pre>output_tensor</code>, 计算出<code v-pre>input_tensor_grad</code>，其\n中<code v-pre>output_tensor</code>已经经过伪释放，只保留了计算图信息，然后通过bwd计算出\n了<code v-pre>input_tensor_grad</code>, 存储在<code v-pre>input_tensor</code>中，我们需要将<code v-pre>input_tensor</code>中\n的<code v-pre>grad</code>取出来，send出去, 之后<code v-pre>input_tensor</code>和<code v-pre>output_tensor_grad</code>本身也就没\n用了，完全销毁掉。</li>\n</ol>\n<p>总结: <code v-pre>input_tensor</code>: 计算完bwd之后完全释放，<code v-pre>output_tensor</code>: 计算完梯度之后完全\n释放，<code v-pre>output_tensor_grad</code>: 计算完梯度之后完全释放，<code v-pre>input_tensor_grad</code>, send之\n后完全释放</p>\n","tagOpen":"<template>","tagClose":"</template>"},"script":null,"scriptSetup":null,"scripts":[],"styles":[],"customBlocks":[]},"content":"本节主要就伪代码来演示pipline parallel如何实现, 主要参考是Megatron-LM中的\ndistribute pp部分\n\n[一篇讲gpipe和pipedream的文章](https://developer.aliyun.com/article/1644595)\n\n在Megatron-LM中，pipline parallel schedule的选择是通\n过`get_forward_backward_func()`来实现的，会根据当前的配置选择适合的调度策略，然\n后完整地进行一次batch的fwd和bwd，因为调度策略分为很多，例如1F1B-interleaved,\ndualpipev..., 所以这里可以学习dualpipe的方式，定义一个通用的Schedule基类, 此后的\n调度策略通过实现这个基类来进行\n\n```python\nclass BaseScheduleStrategy(ABC):\n    \"\"\"\n    Base class for all schedule strategies\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    @classmethod\n    @abstractmethod\n    def initialize_pipline_distributed(cls, *args, **kwargs):\n        \"\"\"\n        Initialize the pipline parallel distributed strategy hooks or utils\n        \"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def get_stage_layer_ids(cls, num_layers: int) -> List[List[int]]:\n        \"\"\"\n        Get the layer ids for each stage\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Call the schedule strategy\n        \"\"\"\n        pass\n```\n\n在实际的pp中，需要考虑更多的细节，例如tensor的释放与回收，这里我们讨论train的\npp，因为对于inference来说，只需要执行fwd的过程，简单了很多。\n\n首先我们需要明确两个点：\n\n1. 在fwd阶段，我们需要的是上一个阶段的`input_tensor`, 然后获\n   得`output_tensor = fwd(input_tensor)`, `output_tensor`发送给下一个stage作\n   为`input_tensor`, 这里`input_tensor`和`output_tensor`都应该压入栈中等待fwd，\n   发送完之后，`output_tensor`的数据其实已经不需要了，因为由下一个stage\n   的`input_tensor`来保存，但是我们仍然需要其`grad_fn`之类的元数据来执行bwd, 所\n   以这里进行deallocate(伪释放)，不是del, 而是替换其数据字段为一个标量。\n\n总结：`input_tensor`: 完全保留，`output_tensor`: 保留grad_fn\n\n2. 在bwd阶段，我们需要的是上一个阶段传来的`output_tensor_grad`, 然后根据当前阶段\n   的`input_tensor`和`output_tensor`, 计算出`input_tensor_grad`，其\n   中`output_tensor`已经经过伪释放，只保留了计算图信息，然后通过bwd计算出\n   了`input_tensor_grad`, 存储在`input_tensor`中，我们需要将`input_tensor`中\n   的`grad`取出来，send出去, 之后`input_tensor`和`output_tensor_grad`本身也就没\n   用了，完全销毁掉。\n\n总结: `input_tensor`: 计算完bwd之后完全释放，`output_tensor`: 计算完梯度之后完全\n释放，`output_tensor_grad`: 计算完梯度之后完全释放，`input_tensor_grad`, send之\n后完全释放","excerpt":"","includedFiles":[],"tasklistId":0,"title":"","headers":[]}}
