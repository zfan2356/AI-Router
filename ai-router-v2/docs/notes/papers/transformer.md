---
title: Transformer论文
author: zfan
createTime: 2025/04/11 22:07:39
permalink: /paper/transformer/
tags:
  - paper
---

[论文地址](https://arxiv.org/pdf/1706.03762)

## 2. 常见问题

这里先放一个链接，之后再完善

https://zhuanlan.zhihu.com/p/496012402?utm_medium=social&utm_oi=629375409599549440

1. 为什么multi-head attention中Q, K, V要先经过一层linear

2.
