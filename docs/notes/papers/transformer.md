---
title: Transformer论文
author: zfan
---


[论文地址](https://arxiv.org/pdf/1706.03762)


## 2. 常见问题

这里先放一个链接，之后再完善

https://zhuanlan.zhihu.com/p/496012402?utm_medium=social&utm_oi=629375409599549440


1. 为什么multi-head attention中Q, K, V要先经过一层linear

2. 