import{_ as a,c as t,b as e,o as n}from"./app-0p9pLXBN.js";const l={};function o(s,r){return n(),t("div",null,r[0]||(r[0]=[e("p",null,[e("a",{href:"https://arxiv.org/pdf/1706.03762",target:"_blank",rel:"noopener noreferrer"},"论文地址")],-1),e("h2",{id:"_2-常见问题",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-常见问题"},[e("span",null,"2. 常见问题")])],-1),e("p",null,"这里先放一个链接，之后再完善",-1),e("p",null,"https://zhuanlan.zhihu.com/p/496012402?utm_medium=social&utm_oi=629375409599549440",-1),e("ol",null,[e("li",null,[e("p",null,"为什么multi-head attention中Q, K, V要先经过一层linear")]),e("li")],-1)]))}const p=a(l,[["render",o]]),m=JSON.parse('{"path":"/paper/transformer/","title":"Transformer论文","lang":"zh-CN","frontmatter":{"title":"Transformer论文","author":"zfan","createTime":"2025/04/11 22:07:39","permalink":"/paper/transformer/","tags":["paper"]},"headers":[],"readingTime":{"minutes":0.21,"words":64},"git":{"updatedTime":1744386529000},"filePathRelative":"notes/papers/transformer.md","categoryList":[{"id":"4358b5","sort":10001,"name":"notes"},{"id":"524d92","sort":10002,"name":"papers"}],"bulletin":false}');export{p as comp,m as data};
